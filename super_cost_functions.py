{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bcefe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from inspect import signature\n",
    "\n",
    "class Setup:\n",
    "    def __init__(self,name,**kwargs):        \n",
    "        ### make cost function choice ###\n",
    "        # for regression\n",
    "        if name == 'least_squares':\n",
    "            self.cost = self.least_squares\n",
    "        if name == 'least_absolute_deviations':\n",
    "            self.cost = self.least_absolute_deviations\n",
    "            \n",
    "        # for two-class classification\n",
    "        if name == 'softmax':\n",
    "            self.cost = self.softmax\n",
    "        if name == 'perceptron':\n",
    "            self.cost = self.perceptron\n",
    "        if name == 'twoclass_counter':\n",
    "            self.cost = self.counting_cost\n",
    "            \n",
    "        # for multiclass classification\n",
    "        if name == 'multiclass_perceptron':\n",
    "            self.cost = self.multiclass_perceptron\n",
    "        if name == 'multiclass_softmax':\n",
    "            self.cost = self.multiclass_softmax\n",
    "        if name == 'multiclass_counter':\n",
    "            self.cost = self.multiclass_counting_cost\n",
    "            \n",
    "        # for autoencoder\n",
    "        if name == 'autoencoder':\n",
    "            self.feature_transforms = feature_transforms\n",
    "            self.feature_transforms_2 = kwargs['feature_transforms_2']\n",
    "            self.cost = self.autoencoder\n",
    "            \n",
    "    ### insert feature transformations to use ###\n",
    "    def define_feature_transform(self,feature_transforms):\n",
    "        # make copy of feature transformation\n",
    "        self.feature_transforms = feature_transforms\n",
    "        \n",
    "        # count parameter layers of input to feature transform\n",
    "        self.sig = signature(self.feature_transforms)\n",
    "            \n",
    "    ##### models functions #####\n",
    "    # compute linear combination of features\n",
    "    def model(self,x,w):   \n",
    "        # feature transformation - switch for dealing\n",
    "        # with feature transforms that either do or do\n",
    "        # not have internal parameters\n",
    "        f = 0\n",
    "        if len(self.sig.parameters) == 2:\n",
    "            f = self.feature_transforms(x,w[0])\n",
    "        else: \n",
    "            f = self.feature_transforms(x)    \n",
    "\n",
    "        # compute linear combination and return\n",
    "        # switch for dealing with feature transforms that either \n",
    "        # do or do not have internal parameters\n",
    "        a = 0\n",
    "        if len(self.sig.parameters) == 2:\n",
    "            a = w[1][0] + np.dot(f.T,w[1][1:])\n",
    "        else:\n",
    "            a = w[0] + np.dot(f.T,w[1:])\n",
    "        return a.T\n",
    "\n",
    "    ###### regression costs #######\n",
    "    # an implementation of the least squares cost function for linear regression\n",
    "    def least_squares(self,w,x,y,iter):\n",
    "        # get batch of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "                \n",
    "        # compute cost over batch\n",
    "        cost = np.sum((self.model(x_p,w) - y_p)**2)\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # a compact least absolute deviations cost function\n",
    "    def least_absolute_deviations(self,w,x,y,iter):\n",
    "        # get batch of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "\n",
    "        # compute cost over batch\n",
    "        cost = np.sum(np.abs(self.model(x_p,w) - y_p))\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    ###### two-class classification costs #######\n",
    "    # the convex softmax cost function\n",
    "    def softmax(self,w,x,y,iter):\n",
    "        # get batch of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "        \n",
    "        # compute cost over batch\n",
    "        cost = np.sum(np.log(1 + np.exp(-y_p*self.model(x_p,w))))\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # the convex relu cost function\n",
    "    def relu(self,w,x,y,iter):\n",
    "        # get batch of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "        \n",
    "        # compute cost over batch\n",
    "        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # the counting cost function\n",
    "    def counting_cost(self,w,x,y):\n",
    "        cost = np.sum((np.sign(self.model(x,w)) - y)**2)\n",
    "        return 0.25*cost \n",
    "\n",
    "    ###### multiclass classification costs #######\n",
    "    # multiclass perceptron\n",
    "    def multiclass_perceptron(self,w,x,y,iter):\n",
    "        # get subset of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "\n",
    "        # pre-compute predictions on all points\n",
    "        all_evals = self.model(x_p,w)\n",
    "\n",
    "        # compute maximum across data points\n",
    "        a =  np.max(all_evals,axis = 0)        \n",
    "\n",
    "        # compute cost in compact form using numpy broadcasting\n",
    "        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n",
    "        cost = np.sum(a - b)\n",
    "\n",
    "        # return average\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # multiclass softmax\n",
    "    def multiclass_softmax(self,w,x,y,iter):     \n",
    "        # get subset of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "        \n",
    "        # pre-compute predictions on all points\n",
    "        all_evals = self.model(x_p,w)\n",
    "\n",
    "        # compute softmax across data points\n",
    "        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n",
    "\n",
    "        # compute cost in compact form using numpy broadcasting\n",
    "        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n",
    "        cost = np.sum(a - b)\n",
    "\n",
    "        # return average\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # multiclass misclassification cost function - aka the fusion rule\n",
    "    def multiclass_counting_cost(self,w,x,y):                \n",
    "        # pre-compute predictions on all points\n",
    "        all_evals = self.model(x,w)\n",
    "\n",
    "        # compute predictions of each input point\n",
    "        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n",
    "\n",
    "        # compare predicted label to actual label\n",
    "        count = np.sum(np.abs(np.sign(y - y_predict)))\n",
    "\n",
    "        # return number of misclassifications\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11c5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
