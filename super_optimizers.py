{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11f3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import value_and_grad \n",
    "from autograd import hessian\n",
    "from autograd.misc.flatten import flatten_func\n",
    "from IPython.display import clear_output\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "\n",
    "# minibatch gradient descent\n",
    "def gradient_descent(g,w,x_train,y_train,x_val,y_val,alpha,max_its,batch_size,**kwargs): \n",
    "    verbose = True\n",
    "    if 'verbose' in kwargs:\n",
    "        verbose = kwargs['verbose']\n",
    "    \n",
    "    # flatten the input function, create gradient based on flat function\n",
    "    g_flat, unflatten, w = flatten_func(g, w)\n",
    "    grad = value_and_grad(g_flat)\n",
    "\n",
    "    # record history\n",
    "    num_train = y_train.size\n",
    "    num_val = y_val.size\n",
    "    w_hist = [unflatten(w)]\n",
    "    train_hist = [g_flat(w,x_train,y_train,np.arange(num_train))]\n",
    "    val_hist = [g_flat(w,x_val,y_val,np.arange(num_val))]\n",
    "\n",
    "    # how many mini-batches equal the entire dataset?\n",
    "    num_batches = int(np.ceil(np.divide(num_train, batch_size)))\n",
    "\n",
    "    # over the line\n",
    "    for k in range(max_its):                   \n",
    "        # loop over each minibatch\n",
    "        start = timer()\n",
    "        train_cost = 0\n",
    "        for b in range(num_batches):\n",
    "            # collect indices of current mini-batch\n",
    "            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_train))\n",
    "            \n",
    "            # plug in value into func and derivative\n",
    "            cost_eval,grad_eval = grad(w,x_train,y_train,batch_inds)\n",
    "            grad_eval.shape = np.shape(w)\n",
    "    \n",
    "            # take descent step with momentum\n",
    "            w = w - alpha*grad_eval\n",
    "\n",
    "        end = timer()\n",
    "        \n",
    "        # update training and validation cost\n",
    "        train_cost = g_flat(w,x_train,y_train,np.arange(num_train))\n",
    "        val_cost = g_flat(w,x_val,y_val,np.arange(num_val))\n",
    "\n",
    "        # record weight update, train and val costs\n",
    "        w_hist.append(unflatten(w))\n",
    "        train_hist.append(train_cost)\n",
    "        val_hist.append(val_cost)\n",
    "\n",
    "        if verbose == True:\n",
    "            print ('step ' + str(k+1) + ' done in ' + str(np.round(end - start,1)) + ' secs, train cost = ' + str(np.round(train_hist[-1][0],4)) + ', val cost = ' + str(np.round(val_hist[-1][0],4)))\n",
    "\n",
    "    if verbose == True:\n",
    "        print ('finished all ' + str(max_its) + ' steps')\n",
    "        #time.sleep(1.5)\n",
    "        #clear_output()\n",
    "    return w_hist,train_hist,val_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec226c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
