{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOJS4unEuwiJ"
   },
   "source": [
    "# Coding CNNs from Scratch with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAglL7KZu3ge"
   },
   "source": [
    "In this assignment you will code a famous CNN architecture AlexNet (https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) to classify images from the CIFAR10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 10 classes of natural images such as vehicles or animals. AlexNet is a landmark architecture because it was one of the first extremely deep CNNs trained on GPUs, and achieved state-of-the-art performance in the ImageNet challenge in 2012.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv77OEtlxuP8"
   },
   "source": [
    "A lot of code will already be written to familiarize yourself with PyTorch, but you will have to fill in parts that will apply your knowledge of CNNs. Additionally, there are some numbered questions that you must answer either in a separate document, or in this notebook. Some questions may require you to do a little research. To type in the notebook, you can insert a text cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr5aNOagwwm5"
   },
   "source": [
    "Let's start by installing PyTorch and the torchvision package below. Due to the size of the network, you will have to run on a GPU. So, click on the Runtime dropdown, then Change Runtime Type, then GPU for the hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXnfRg4IulGd",
    "outputId": "c7331937-5d43-4fef-ea98-6c836ca2c5c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: pytorch\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.1+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtC0KJcdufBE",
    "outputId": "45ed863c-d9e5-4db2-870a-649e5299b4c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DML-S0AX-_o"
   },
   "source": [
    "### 1. In the following cell, we are employing something called \"data augmentation\" with random horizontal and vertical flips. So when training data is fed into the network, it is ranadomly transformed. What are advantages of this?\n",
    "  This reduces overfitting of data by exposing it to additional variations of the same image. This assumes that we can get more information from the original dataset by using augmentation which can also be helpful if we are training with a limited dataset. This overall improves the model accuracy.\n",
    "\n",
    "### 2. We normalize with the line transforms.Normalize((0.5,), (0.5,)). What are the benefits of normalizing data?\n",
    "Normalization can help training of our neural networks as this step gets the different features are on a similar scale, which helps to stabilize the gradient descent step, allowing the use of larger learning rates or helping models converge faster for a given learning rate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eruiC4sAufBL",
    "outputId": "3447649e-c705-4e39-f156-2aff02df0e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "40000 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from math import ceil\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomVerticalFlip(p=0.5),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "torch.manual_seed(43)\n",
    "val_size = 10000\n",
    "train_size = len(trainset) - val_size\n",
    "\n",
    "\n",
    "train_ds, val_ds = random_split(trainset, [train_size, val_size])\n",
    "print(len(train_ds), len(val_ds))\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "num_steps =  ceil(len(train_ds) / BATCH_SIZE)\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "NLzuKuJxufBM"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(testset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rT3aDd7aVLm"
   },
   "source": [
    "You can insert an integer  into the code trainset[#insert integer] to visualize images from the training set. Some of the images might look weird because they have been randomly flipped according to our data augmentation scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "wV-W2b6eZaoG",
    "outputId": "b6a2c70f-8917-4ddc-fc00-eb4ea5693938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label (numeric): 3\n",
      "Label (textual): cat\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = trainset[9]\n",
    "plt.imshow((img.permute((1, 2, 0))+1)/2)\n",
    "print('Label (numeric):', label)#`vbn. z wertjkwertyu;\n",
    "print('Label (textual):', classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVLQM7ODamEm"
   },
   "source": [
    "Now comes the fun part. You will have to put in the correct parameters into different torch.nn functions in order to convolve and downsample the image into the correct dimensionality for classification. Think of it as a puzzle. You will insert the parameters where there is a comment #TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ohY_5zoBufBN"
   },
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3,#TODO\n",
    "                      out_channels = 64,#TODO\n",
    "                      kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3,\n",
    "                      padding=1),#TODO\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192,#TODO\n",
    "                      384,#TODO\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        #Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024,4096),#TODO),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),#TODO),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096,100), #TODO),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #we must flatten our feature maps before feeding into fully connected layers\n",
    "        x = x.contiguous().view(x.size(0),-1) #TODO)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw2ZfQfAeZum"
   },
   "source": [
    "Below we are initializing our model with a weight scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlUFRnhZufBN",
    "outputId": "f2f07a78-23ec-4f56-c1ac-b0ad28b8e48d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Discriminator()\n",
    "\n",
    "def weights_init(m):\n",
    "\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "# Initialize Models\n",
    "net = net.to(device)\n",
    "\n",
    "net.apply(weights_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbGFZBgMPJXz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFBRnApFfZUr"
   },
   "source": [
    "# 3. Notice above in our network architecture, we have what are called \"Dropout\" layers. What is the point of these?\n",
    "\n",
    "Dropout layers are adapted to prevent the issue of overfitting which is where the model may learn statistical noise. Without dropouts, units may learn to fix mistakes of other units thereby creating co-adaptations leading to our model not getting generalized. Addition of dropout layers forces layers to take more or less responsibility for the inputs with a probabilistic approach therebt making the model more general resulting in a better performance with test dataset it has never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqzRwTN-febi"
   },
   "source": [
    "Defining our cost/loss function, which is cross-entropy loss. We also define our optimizer with hyperparameters: learning rate and betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "sNvPJc_PufBN"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=0.0002,\n",
    "    betas = (0.5, 0.999)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR6xLm7KiR3p"
   },
   "source": [
    "Below we actually train our network. Run for just 10 epochs. It takes some time. Wherever there is the comment #TODO, you must insert code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VwEjNs3ufBO",
    "outputId": "55ac8c0f-86b6-474f-ff62-092eff52936a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:1, Train Loss:1.4220407459139823\n",
      "Accuracy of 10000 val images: 0.5092\n",
      "Val Loss: 0.3378035110235214\n",
      "E:2, Train Loss:1.2951746878027917\n",
      "Accuracy of 10000 val images: 0.5216\n",
      "Val Loss: 0.32890733271837236\n",
      "E:3, Train Loss:1.1976687783002853\n",
      "Accuracy of 10000 val images: 0.5603\n",
      "Val Loss: 0.3034729914367199\n",
      "E:4, Train Loss:1.1180827513337135\n",
      "Accuracy of 10000 val images: 0.5948\n",
      "Val Loss: 0.2820093309879303\n",
      "E:5, Train Loss:1.0553837290406227\n",
      "Accuracy of 10000 val images: 0.6151\n",
      "Val Loss: 0.26997333616018293\n",
      "E:6, Train Loss:0.9979504333436489\n",
      "Accuracy of 10000 val images: 0.6319\n",
      "Val Loss: 0.2598070977628231\n",
      "E:7, Train Loss:0.9434045785665512\n",
      "Accuracy of 10000 val images: 0.6278\n",
      "Val Loss: 0.256942468136549\n",
      "E:8, Train Loss:0.891601822078228\n",
      "Accuracy of 10000 val images: 0.6537\n",
      "Val Loss: 0.2474515427649021\n",
      "E:9, Train Loss:0.8506807227432728\n",
      "Accuracy of 10000 val images: 0.6483\n",
      "Val Loss: 0.25523970171809196\n",
      "E:10, Train Loss:0.8076005019247532\n",
      "Accuracy of 10000 val images: 0.6567\n",
      "Val Loss: 0.2526149518787861\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device) #TODO\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)#TODDO     #pass input data into network to get outputs\n",
    "        loss = criterion(outputs,labels)#TODO)\n",
    "        loss.backward()  #calculate gradients\n",
    "        optimizer.step() #take gradient descent step\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(\"E:{}, Train Loss:{}\".format(\n",
    "                epoch+1,\n",
    "                running_loss / num_steps\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    #validation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            #TODO: load images and labels from validation loader\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)#TODO  #run forward pass\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs,labels)    #TODO       #calculate validation loss\n",
    "            val_loss += loss.item()\n",
    "    val_loss /=num_steps\n",
    "    print('Accuracy of 10000 val images: {}'.format( correct / total))\n",
    "    print('Val Loss: {}'.format( val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sKf8Lu-mGYd"
   },
   "source": [
    "## 4. If we train for more epochs, our accuracy/performance will increase. What happens if we train for too long though? What method can be employed to mitigate this?\n",
    "\n",
    "If we train for too long, we again run into the issue of overfitting. The network could end up hard encoding some biases and weights by learning the training data to the point it stops being able to generalize and ends up performing poorly in the validation tests. The data augmentation and dropout method we used are few of the tools used to counter overfitting. Another thing we could do is monitor the accuracy in both testing and training data as a function of epochs and see that it performs well in both. We could also set an upper bound on the number of epochs once a desired value of accuracy is reached. If not, we might have to readjust our model, maybe relying on an ensemble of models so there is less chance of hard encoding some of the biases into one specific model.\n",
    "\n",
    "\n",
    "## 5. Try increasing learning rate and look at the metrics for training and validation data? What do you notice? Why do think this is happening?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz31Vm2XmM7p"
   },
   "source": [
    "We can see the performance on the testing set now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7rWk5FajqUr"
   },
   "source": [
    "Changing learning rate from 0.0002 to 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0DrrE5GlkDX"
   },
   "source": [
    "Since ADAM computes individual adaptive learning rates with the learning rate sent while calling the function as the bound, we want to start with the smallest possible learning rate and increase it while the validation loss as a function of learning rate decreses. The optimal learning rate would be a rate a little below the point the validation loss starts climbing up with increase in learning rate.\n",
    "\n",
    "In the run below with learning rate 0.01, we see that the rate is too high and since the step size is too large, the model might be missing the points at which the cost is minimized. Since the learning rate is already too high, one way to find the optimal learning rate would be to restart the model, and look for the optimal value we just discussed about by plotting validation loss as a function of learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "oF84O0ZSjT-s"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=0.01,\n",
    "    betas = (0.5, 0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNLMA4oIufBO",
    "outputId": "847f1398-23fc-4793-815d-c071b0c21d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10000 test images: 0.1\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of 10000 test images: {}'.format( correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYGg6BRbiwEL",
    "outputId": "7dca1dcc-7ecc-455c-f3d5-da90d14bf4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:1, Train Loss:71.25373828664422\n",
      "Accuracy of 10000 val images: 0.1589\n",
      "Val Loss: 0.5949645668268204\n",
      "E:2, Train Loss:2.192418440878391\n",
      "Accuracy of 10000 val images: 0.1903\n",
      "Val Loss: 0.5358748480677604\n",
      "E:3, Train Loss:2.2002488535642626\n",
      "Accuracy of 10000 val images: 0.2025\n",
      "Val Loss: 0.525718737244606\n",
      "E:4, Train Loss:2.0801099979877473\n",
      "Accuracy of 10000 val images: 0.1983\n",
      "Val Loss: 0.5208989408612251\n",
      "E:5, Train Loss:2.0226137349009514\n",
      "Accuracy of 10000 val images: 0.1658\n",
      "Val Loss: 0.5497078382968903\n",
      "E:6, Train Loss:3.3257417610287665\n",
      "Accuracy of 10000 val images: 0.1022\n",
      "Val Loss: 0.5867826229333878\n",
      "E:7, Train Loss:2.3256529533863066\n",
      "Accuracy of 10000 val images: 0.0959\n",
      "Val Loss: 0.5786735653877259\n",
      "E:8, Train Loss:2.3115513783693316\n",
      "Accuracy of 10000 val images: 0.0956\n",
      "Val Loss: 0.5797243237495422\n",
      "E:9, Train Loss:2.308466332554817\n",
      "Accuracy of 10000 val images: 0.0966\n",
      "Val Loss: 0.5768873572349549\n",
      "E:10, Train Loss:2.3086473774909972\n",
      "Accuracy of 10000 val images: 0.0966\n",
      "Val Loss: 0.5764306473731995\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device) #TODO\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)#TODDO     #pass input data into network to get outputs\n",
    "        loss = criterion(outputs,labels)#TODO)\n",
    "        loss.backward()  #calculate gradients\n",
    "        optimizer.step() #take gradient descent step\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(\"E:{}, Train Loss:{}\".format(\n",
    "                epoch+1,\n",
    "                running_loss / num_steps\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    #validation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            #TODO: load images and labels from validation loader\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)#TODO  #run forward pass\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs,labels)    #TODO       #calculate validation loss\n",
    "            val_loss += loss.item()\n",
    "    val_loss /=num_steps\n",
    "    print('Accuracy of 10000 val images: {}'.format( correct / total))\n",
    "    print('Val Loss: {}'.format( val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmSZ3dNbnpPA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
=======
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasmineA20/435-deep-learning/blob/main/EE435_PyTorch_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOJS4unEuwiJ"
      },
      "source": [
        "# Coding CNNs from Scratch with Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAglL7KZu3ge"
      },
      "source": [
        "In this assignment you will code a famous CNN architecture AlexNet (https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) to classify images from the CIFAR10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 10 classes of natural images such as vehicles or animals. AlexNet is a landmark architecture because it was one of the first extremely deep CNNs trained on GPUs, and achieved state-of-the-art performance in the ImageNet challenge in 2012.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv77OEtlxuP8"
      },
      "source": [
        "A lot of code will already be written to familiarize yourself with PyTorch, but you will have to fill in parts that will apply your knowledge of CNNs. Additionally, there are some numbered questions that you must answer either in a separate document, or in this notebook. Some questions may require you to do a little research. To type in the notebook, you can insert a text cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr5aNOagwwm5"
      },
      "source": [
        "Let's start by installing PyTorch and the torchvision package below. Due to the size of the network, you will have to run on a GPU. So, click on the Runtime dropdown, then Change Runtime Type, then GPU for the hardware accelerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXnfRg4IulGd"
      },
      "source": [
        "!pip install pytorch\n",
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtC0KJcdufBE"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DML-S0AX-_o"
      },
      "source": [
        "### 1. In the following cell, we are employing something called \"data augmentation\" with random horizontal and vertical flips. So when training data is fed into the network, it is ranadomly transformed. What are advantages of this?\n",
        "\n",
        "### 2. We normalize with the line transforms.Normalize((0.5,), (0.5,)). What are the benefits of normalizing data?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eruiC4sAufBL"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "from math import ceil\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.RandomHorizontalFlip(p=0.5),\n",
        "     transforms.RandomVerticalFlip(p=0.5),\n",
        "     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "torch.manual_seed(43)\n",
        "val_size = 10000\n",
        "train_size = len(trainset) - val_size\n",
        "\n",
        "\n",
        "train_ds, val_ds = random_split(trainset, [train_size, val_size])\n",
        "print(len(train_ds), len(val_ds))\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "num_steps =  ceil(len(train_ds) / BATCH_SIZE)\n",
        "num_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLzuKuJxufBM"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, BATCH_SIZE, shuffle=True, drop_last = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, BATCH_SIZE)\n",
        "test_loader = torch.utils.data.DataLoader(testset, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3aDd7aVLm"
      },
      "source": [
        "You can insert an integer  into the code trainset[#insert integer] to visualize images from the training set. Some of the images might look weird because they have been randomly flipped according to our data augmentation scheme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "wV-W2b6eZaoG",
        "outputId": "59486fe1-2667-4bf1-b7af-84a647899b83"
      },
      "source": [
        "img, label = trainset[#insert integer]\n",
        "plt.imshow((img.permute((1, 2, 0))+1)/2)\n",
        "print('Label (numeric):', label)\n",
        "print('Label (textual):', classes[label])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label (numeric): 1\n",
            "Label (textual): car\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVLQM7ODamEm"
      },
      "source": [
        "Now comes the fun part. You will have to put in the correct parameters into different torch.nn functions in order to convolve and downsample the image into the correct dimensionality for classification. Think of it as a puzzle. You will insert the parameters where there is a comment #TODO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohY_5zoBufBN"
      },
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = #TODO,\n",
        "                      out_channels = #TODO,\n",
        "                      kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3,\n",
        "                      padding=#TODO),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(#TODO,\n",
        "                      #TODO,\n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        #Fully connected layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(#TODO),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, #TODO),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, #TODO),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        #we must flatten our feature maps before feeding into fully connected layers\n",
        "        x = x.contiguous().view(x.size(0), #TODO)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw2ZfQfAeZum"
      },
      "source": [
        "Below we are initializing our model with a weight scheme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlUFRnhZufBN"
      },
      "source": [
        "net = Discriminator()\n",
        "\n",
        "def weights_init(m):\n",
        "\n",
        "    classname = m.__class__.__name__\n",
        "\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "# Initialize Models\n",
        "net = net.to(device)\n",
        "\n",
        "net.apply(weights_init)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFBRnApFfZUr"
      },
      "source": [
        "# 3. Notice above in our network architecture, we have what are called \"Dropout\" layers. What is the point of these?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqzRwTN-febi"
      },
      "source": [
        "Defining our cost/loss function, which is cross-entropy loss. We also define our optimizer with hyperparameters: learning rate and betas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNvPJc_PufBN"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    net.parameters(),\n",
        "    lr=0.0002,\n",
        "    betas = (0.5, 0.999)\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR6xLm7KiR3p"
      },
      "source": [
        "Below we actually train our network. Run for just 10 epochs. It takes some time. Wherever there is the comment #TODO, you must insert code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VwEjNs3ufBO"
      },
      "source": [
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = #TODO\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = #TODO     #pass input data into network to get outputs\n",
        "        loss = criterion(#TODO)\n",
        "        loss.backward()  #calculate gradients\n",
        "        optimizer.step() #take gradient descent step\n",
        "\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "    print(\"E:{}, Train Loss:{}\".format(\n",
        "                epoch+1,\n",
        "                running_loss / num_steps\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    #validation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            #TODO: load images and labels from validation loader\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = #TODO  #run forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            loss = #TODO       #calculate validation loss\n",
        "            val_loss += loss.item()\n",
        "    val_loss /=num_steps\n",
        "    print('Accuracy of 10000 val images: {}'.format( correct / total))\n",
        "    print('Val Loss: {}'.format( val_loss))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sKf8Lu-mGYd"
      },
      "source": [
        "## 4. If we train for more epochs, our accuracy/performance will increase. What happens if we train for too long though? What method can be employed to mitigate this?\n",
        "\n",
        "## 5. Try increasing learning rate and look at the metrics for training and validation data? What do you notice? Why do think this is happening?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz31Vm2XmM7p"
      },
      "source": [
        "We can see the performance on the testing set now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNLMA4oIufBO"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of 10000 test images: {}'.format( correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
>>>>>>> 84317fb4b771dab08b7319ccdedfc2db5f4bf3a0
