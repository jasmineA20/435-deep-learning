{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4981574",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m super_optimizers \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m super_cost_functions\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalizers\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from . import super_optimizers \n",
    "from . import super_cost_functions\n",
    "from . import normalizers\n",
    "from . import multilayer_perceptron\n",
    "from . import multilayer_perceptron_batch_normalized\n",
    "from . import history_plotters\n",
    "\n",
    "class Setup:\n",
    "    def __init__(self,x,y,**kwargs):\n",
    "        # link in data\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        # make containers for all histories\n",
    "        self.weight_histories = []\n",
    "        self.train_cost_histories = []\n",
    "        self.train_accuracy_histories = []\n",
    "        self.val_cost_histories = []\n",
    "        self.val_accuracy_histories = []\n",
    "        self.train_costs = []\n",
    "        self.train_counts = []\n",
    "        self.val_costs = []\n",
    "        self.val_counts = []\n",
    "        \n",
    "    #### define preprocessing steps ####\n",
    "    def preprocessing_steps(self,**kwargs):        \n",
    "        ### produce / use data normalizer ###\n",
    "        normalizer_name = 'standard'\n",
    "        if 'normalizer_name' in kwargs:\n",
    "            normalizer_name = kwargs['normalizer_name']\n",
    "        self.normalizer_name = normalizer_name\n",
    "\n",
    "        # produce normalizer / inverse normalizer\n",
    "        s = normalizers.Setup(self.x,normalizer_name)\n",
    "        self.normalizer = s.normalizer\n",
    "        self.inverse_normalizer = s.inverse_normalizer\n",
    "        \n",
    "        # normalize input \n",
    "        self.x = self.normalizer(self.x)\n",
    "       \n",
    "    #### split data into training and validation sets ####\n",
    "    def make_train_val_split(self,train_portion):\n",
    "        # translate desired training portion into exact indecies\n",
    "        self.train_portion = train_portion\n",
    "        r = np.random.permutation(self.x.shape[1])\n",
    "        train_num = int(np.round(train_portion*len(r)))\n",
    "        self.train_inds = r[:train_num]\n",
    "        self.val_inds = r[train_num:]\n",
    "        \n",
    "        # define training and testing sets\n",
    "        self.x_train = self.x[:,self.train_inds]\n",
    "        self.x_val = self.x[:,self.val_inds]\n",
    "\n",
    "        self.y_train = self.y[:,self.train_inds]\n",
    "        self.y_val = self.y[:,self.val_inds]\n",
    "     \n",
    "    #### define cost function ####\n",
    "    def choose_cost(self,name,**kwargs):\n",
    "        # create training and testing cost functions\n",
    "        self.cost_object = super_cost_functions.Setup(name,**kwargs)\n",
    "\n",
    "        # if the cost function is a two-class classifier, build a counter too\n",
    "        if name == 'softmax' or name == 'perceptron':\n",
    "            self.count_object = super_cost_functions.Setup('twoclass_counter',**kwargs)\n",
    "                        \n",
    "        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n",
    "            self.count_object = super_cost_functions.Setup('multiclass_counter',**kwargs)\n",
    "  \n",
    "        self.cost_name = name\n",
    "    \n",
    "    #### define feature transformation ####\n",
    "    def choose_features(self,**kwargs): \n",
    "        ### select from pre-made feature transforms ###\n",
    "        layer_sizes = [1]\n",
    "        if 'layer_sizes' in kwargs:\n",
    "            layer_sizes = kwargs['layer_sizes']\n",
    "        \n",
    "        # add input and output layer sizes\n",
    "        input_size = self.x.shape[0]\n",
    "        layer_sizes.insert(0, input_size)\n",
    "      \n",
    "        # add output size\n",
    "        if self.cost_name == 'least_squares' or self.cost_name == 'least_absolute_deviations':\n",
    "            layer_sizes.append(self.y.shape[0])\n",
    "        else:\n",
    "            num_labels = len(np.unique(self.y))\n",
    "            if num_labels == 2:\n",
    "                layer_sizes.append(1)\n",
    "            else:\n",
    "                layer_sizes.append(num_labels)\n",
    "        \n",
    "        # multilayer perceptron #\n",
    "        feature_name = 'multilayer_perceptron'\n",
    "        if 'name' in kwargs:\n",
    "            feature_name = kwargs['feature_name']\n",
    "           \n",
    "        if feature_name == 'multilayer_perceptron':\n",
    "            transformer = multilayer_perceptron.Setup(**kwargs)\n",
    "            self.feature_transforms = transformer.feature_transforms\n",
    "            self.multilayer_initializer = transformer.initializer\n",
    "            self.layer_sizes = transformer.layer_sizes\n",
    "            \n",
    "        if feature_name == 'multilayer_perceptron_batch_normalized':\n",
    "            transformer = multilayer_perceptron_batch_normalized.Setup(**kwargs)\n",
    "            self.feature_transforms = transformer.feature_transforms\n",
    "            self.multilayer_initializer = transformer.initializer\n",
    "            self.layer_sizes = transformer.layer_sizes\n",
    "            \n",
    "        self.feature_name = feature_name\n",
    "        \n",
    "        ### with feature transformation constructed, pass on to cost function ###\n",
    "        self.cost_object.define_feature_transform(self.feature_transforms)\n",
    "        self.cost = self.cost_object.cost\n",
    "        self.model = self.cost_object.model\n",
    "        \n",
    "        # if classification performed, inject feature transforms into counter as well\n",
    "        if self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n",
    "            self.count_object.define_feature_transform(self.feature_transforms)\n",
    "            self.counter = self.count_object.cost\n",
    "            \n",
    "    #### run optimization ####\n",
    "    def fit(self,**kwargs):\n",
    "        # basic parameters for gradient descent run (default algorithm)\n",
    "        max_its = 500; alpha_choice = 10**(-1);\n",
    "        \n",
    "        # set parameters by hand\n",
    "        if 'max_its' in kwargs:\n",
    "            self.max_its = kwargs['max_its']\n",
    "        if 'alpha_choice' in kwargs:\n",
    "            self.alpha_choice = kwargs['alpha_choice']\n",
    "        \n",
    "        # set initialization\n",
    "        self.w_init = self.multilayer_initializer()\n",
    "        \n",
    "        # batch size for gradient descent?\n",
    "        self.train_num = np.size(self.y_train)\n",
    "        self.val_num = np.size(self.y_val)\n",
    "        self.batch_size = np.size(self.y_train)\n",
    "        if 'batch_size' in kwargs:\n",
    "            self.batch_size = min(kwargs['batch_size'],self.batch_size)\n",
    "        \n",
    "        # verbose or not\n",
    "        verbose = True\n",
    "        if 'verbose' in kwargs:\n",
    "            verbose = kwargs['verbose']\n",
    "\n",
    "        # optimize\n",
    "        weight_history = []\n",
    "        cost_history = []\n",
    "        \n",
    "        # run gradient descent\n",
    "        weight_history,train_cost_history,val_cost_history = super_optimizers.gradient_descent(self.cost,self.w_init,self.x_train,self.y_train,self.x_val,self.y_val,self.alpha_choice,self.max_its,self.batch_size,verbose=verbose)\n",
    "                                                                                         \n",
    "        # store all new histories\n",
    "        self.weight_histories.append(weight_history)\n",
    "        self.train_cost_histories.append(train_cost_history)\n",
    "        self.val_cost_histories.append(val_cost_history)\n",
    "\n",
    "        # if classification produce count history\n",
    "        if self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n",
    "            train_accuracy_history = [1 - self.counter(v,self.x_train,self.y_train)/float(self.y_train.size) for v in weight_history]\n",
    "            val_accuracy_history = [1 - self.counter(v,self.x_val,self.y_val)/float(self.y_val.size) for v in weight_history]\n",
    "\n",
    "            # store count history\n",
    "            self.train_accuracy_histories.append(train_accuracy_history)\n",
    "            self.val_accuracy_histories.append(val_accuracy_history)\n",
    " \n",
    "    #### plot histories ###\n",
    "    def show_histories(self,**kwargs):\n",
    "        start = 0\n",
    "        if 'start' in kwargs:\n",
    "            start = kwargs['start']\n",
    "        if self.train_portion == 1:\n",
    "            self.val_cost_histories = [[] for s in range(len(self.val_cost_histories))]\n",
    "            self.val_accuracy_histories = [[] for s in range(len(self.val_accuracy_histories))]\n",
    "        history_plotters.Setup(self.train_cost_histories,self.train_accuracy_histories,self.val_cost_histories,self.val_accuracy_histories,start)\n",
    "        \n",
    "    #### for batch normalized multilayer architecture only - set normalizers to desired settings ####\n",
    "    def fix_normalizers(self,w):\n",
    "        ### re-set feature transformation ###        \n",
    "        # fix normalization at each layer by passing data and specific weight through network\n",
    "        self.feature_transforms(self.x,w);\n",
    "        \n",
    "        # re-assign feature transformation based on these settings\n",
    "        self.testing_feature_transforms = self.transformer.testing_feature_transforms\n",
    "        \n",
    "        ### re-assign cost function (and counter) based on fixed architecture ###\n",
    "        funcs = cost_functions.Setup(self.cost_name,self.x,self.y,self.testing_feature_transforms)\n",
    "        self.model = funcs.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a785f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
